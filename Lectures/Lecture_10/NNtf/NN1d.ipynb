{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9380411b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import activations\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.optimizers import Adam\n",
    "# for 2nd attempt\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "import keras.backend as K"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fad21d9",
   "metadata": {},
   "source": [
    "# Fitting a 1d function with a deep neural network\n",
    "\n",
    "## Aim\n",
    "\n",
    "To find the the best architecture and training setup to fit a pre-defined function\n",
    "\n",
    "$f(x) = 10\\sin(10 x) + (x-1)(x+2)(x-1/2)+80 e^{-10*(x-2)^2} $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e4214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's choose a simple 1->1 function to train\n",
    "def f(x):\n",
    "    return 10*np.sin(10*x)+(x-3)*(x+2)*(x-1/2)+80*np.exp(-(x-2)**2/0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc0a76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate some data points\n",
    "datasize = 500\n",
    "datapoints = np.linspace(-5,5,datasize)\n",
    "datavalues = f(datapoints)\n",
    "data = np.transpose([datapoints,datavalues])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc1bd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data points to see how they are distributed\n",
    "plt.plot(datapoints, datavalues, 'bo')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8615d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## split into testing and training sets\n",
    "split = int(len(data)*80/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d9069b",
   "metadata": {},
   "source": [
    "The output range is not between 0 and 1 so if we were to use a tanh/sigmoid activation function it would fail. We can process the output data using\n",
    "\n",
    "$\\hat{f}_i = \\frac{f_i-f_{\\rm min}}{f_{\\rm max}-f_{min}}$\n",
    "\n",
    "which lies between 0 and 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8dfea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure all data is positive\n",
    "def process_data(data):\n",
    "    data_in_array = data[:,0]\n",
    "    #data_in_ave = np.ave(data_in_array)\n",
    "    #data_in_max = np.max(data_in_array)\n",
    "    #data_in_min = np.min(data_in_array)\n",
    "    #data_in_std = np.std(data_in_array)\n",
    "    \n",
    "    data_out_array = data[:,1]\n",
    "    #data_out_ave = np.ave(data_out_array)\n",
    "    data_out_max = np.max(data_out_array)\n",
    "    data_out_min = np.min(data_out_array)\n",
    "    #data_out_std = np.std(data_out_array)\n",
    "    \n",
    "    data_out_array = (data_out_array-data_out_min)/(data_out_max-data_out_min)\n",
    "    \n",
    "    data_processed = np.transpose([data_in_array,data_out_array])\n",
    "    return data_processed, data_out_min, data_out_max\n",
    "\n",
    "def unprocess_prediction(values, ymin, ymax):\n",
    "    return values*(ymax-ymin) + ymin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb1be65",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbb83a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise model: 1-16-32-16-1\n",
    "model = Sequential([\n",
    "    Dense(16, activation='sigmoid', input_shape=(1,)),\n",
    "    Dense(32, activation='sigmoid'),\n",
    "    Dense(16, activation='sigmoid'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(\n",
    "    optimizer = Adam(learning_rate=0.1, beta_1=0.9, beta_2=0.999, amsgrad=False),\n",
    "    loss = 'mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47209b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processed, ymin, ymax = process_data(data)\n",
    "print(ymin, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81605920",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data[0])\n",
    "print(data_processed[0])\n",
    "\n",
    "plt.plot(data_processed[:,0], data_processed[:,1], 'bo')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7145dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ES = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    min_delta=0,\n",
    "    patience=1000,\n",
    "    verbose=0,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "history = model.fit(\n",
    "    data_processed[0:split,0],\n",
    "    data_processed[0:split,1],\n",
    "    epochs=200,\n",
    "    validation_data=(data_processed[split:,0], data_processed[split:,1]),\n",
    "    #callbacks=[ES],\n",
    "    #batch_size=512,\n",
    "    verbose=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc583003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can plot the history to see how the loss function for the\n",
    "# training and validation set changed with epoch\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.xlim([1,200])\n",
    "plt.ylim([0,0.05])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4492f83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints2 = np.random.uniform(low=-5, high=5, size=(1000,))\n",
    "datavalues2 = np.array(list(map(f, datapoints2)))\n",
    "valuespred = model.predict(datapoints2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdc63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datapoints2, valuespred, 'go')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a87833",
   "metadata": {},
   "outputs": [],
   "source": [
    "valuespred_un = unprocess_prediction(valuespred, ymin, ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f577a82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(datapoints2, datavalues2, 'bo')\n",
    "plt.plot(datapoints2, valuespred_un, 'go')\n",
    "plt.xlim([-5,5])\n",
    "plt.ylabel('f')\n",
    "plt.xlabel('x');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d63e88",
   "metadata": {},
   "source": [
    "## Exercises ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94694286",
   "metadata": {},
   "source": [
    "1) Edit the code to create an arbitrary archictecture from a list of layer depths. How does the fit depend on the architecture?\n",
    "\n",
    "2) Change the activation function used in the nodes to tanh, what changes in the output? Do you need to change the normalisation?\n",
    "\n",
    "3) Vary the number of input data points. Is there an optimal number?\n",
    "\n",
    "4) Using the template for early stopping try to optimise the number of epochs needed by the network\n",
    "\n",
    "https://keras.io/api/callbacks/early_stopping/\n",
    "\n",
    "5) How does batch size affect the training time and fit?\n",
    "\n",
    "6) Split the input and output data set according to random shuffle rather than a slice, do you see any affect? (see sklearn.model_selection.train_test_split).\n",
    "\n",
    "7) Can we change the loss function to prefer certain regions or features? (Only think about this - no need to implement it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d3795",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

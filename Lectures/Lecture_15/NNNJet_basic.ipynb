{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 08:59:53.805832: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from pstools.rambo import dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from tensorflow.keras import activations\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import backend \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Neural Network to approximate amplitudes from pre-generated data ##\n",
    "\n",
    "The sqaured scattering amplitude (summed and averaged over colour) for $e^+e^- \\to$ jets have been evaluated over a flat phase-space generated with the RAMBO algorithm. Various phase-space cuts for the separation of final state 'jets' have been applied using the JADE algorithm. Tree/zero-loop '0L' ($|A^{(0)}|^2$) and one-loop ($2{\\rm Re}(A^{(0)*}A^{(1)}|$) amplitudes have been compute using the NJet C++ library (https://bitbucket.org/njet/njet/wiki/Home)\n",
    "\n",
    "The data consists of 1M phase-space points for processes with 3 or 4 final state jets. The data is given in numpy '.npz' format generated using the 'numpy.savez' command and are labelled as\n",
    "\n",
    "<code>data/NJdata_\\<loop order\\>_ee\\<n\\>j_d\\<jade cut\\>.npz</code>\n",
    "\n",
    "where loop order=0L,1L, n=3,4 and jade cut=0.03, 0.02 and 0.01\n",
    "\n",
    "The data files can be obtained from the url (using wget for example):\n",
    "\n",
    "`wget http://personalpages.to.infn.it/~badger/njet-data/eejet_data.tar.gz`\n",
    "\n",
    "# Aims #\n",
    "\n",
    "To use the tensorflow library to make a simple approximation of the multi-variable amplitude function and test how good of an approximation we get. Try to determine how well we approximate the amplitude point-by-point correlates with the observable cross section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Separate amplitude data and train TensorFlow neural network #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "loop = 0\n",
    "jets = 3\n",
    "delta_cut = 0.03"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['momenta', 'NJ_vals']\n"
     ]
    }
   ],
   "source": [
    "NJdata = np.load(\"data/NJdata_\"+str(loop)+\"L_ee\"+str(jets)+\"j_d\"+str(delta_cut)+\".npz\")\n",
    "print(NJdata.files)\n",
    "\n",
    "mom_data = NJdata['momenta']\n",
    "amp_data = NJdata['NJ_vals']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 1000000 data points\n",
      "will train/validate on 100000 then test on 100000\n"
     ]
    }
   ],
   "source": [
    "# choose the number of training points (will later be split into NN train/validation set)\n",
    "n_points = len(amp_data)\n",
    "print(\"found\", n_points, \"data points\")\n",
    "\n",
    "n_training_points = 100000\n",
    "# choose the number of points for interpolation tests after training\n",
    "# NB - different from the training/validation split during training\n",
    "n_test_points = 100000\n",
    "\n",
    "print(\"will train/validate on\",n_training_points,\"then test on\",n_test_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB the points are already randomly distributed according to RAMBO\n",
    "# so we may just slice the array into training/validation and testing\n",
    "mom_train = mom_data[:n_training_points]\n",
    "amp_train = amp_data[:n_training_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_and_std(myarray, axis=0):\n",
    "    return np.mean(myarray, axis=axis), np.std(myarray, axis=axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(myarray, ave, std):\n",
    "    return (myarray-ave)/std\n",
    "\n",
    "def destandardize(myarray, ave, std):\n",
    "    return myarray*std+ave"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standardized momenta cuts out the initial state since it is fixed for e+e- collisions\n",
    "# NB if we attempted to keep it then dividing by the standard deviation would give division by zero\n",
    "# momenta[:,2:,:] means take all phase space points but only take momenta 2 to n\n",
    "# since the array is labelled momenta[i=(1,n_points), j=(1,n_particles), mu=(1,4)]  \n",
    "mom_ave, mom_std = mean_and_std(mom_train[:,2:,:], axis=0)\n",
    "mom_stdized = standardize(mom_train[:,2:,:], mom_ave, mom_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alternatively we could use momentum invariants (p_i+p_j+...)^2 as inputs\n",
    "# 3n-10 of these are independent for an n-particle scattering (masseless) amplitude\n",
    "# \n",
    "def minkowski_dot_matrix(mm):\n",
    "    g = np.array([1, -1, -1, -1])\n",
    "    mm_tilde = mm * g\n",
    "    A = np.einsum('ik,jk->ij', mm_tilde, mm)\n",
    "\n",
    "    return A\n",
    "\n",
    "def computeinvariants(mm):\n",
    "    A = minkowski_dot_matrix(mm)\n",
    "    inds = np.triu_indices_from(A, k=1)\n",
    "    \n",
    "    return A[inds]\n",
    "\n",
    "inv_train = np.array([computeinvariants(mm) for mm in mom_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# as we are have e+e- with fixed centre-of-mass energy the invariant (p0+p1)^2 = 2*p0.p1 is constant and can be eliminated\n",
    "inv_ave, inv_std = mean_and_std(inv_train[:,1:],axis=0)\n",
    "inv_stdized = standardize(inv_train[:,1:], inv_ave, inv_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the standardized the ampltide values as well\n",
    "amp_ave = np.mean(amp_train)\n",
    "amp_std = np.std(amp_train)\n",
    "amp_stdized = (amp_train-amp_ave)/amp_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now set up Keras model with flattened momenta and/or invariants as input values\n",
    "n_final = len(mom_stdized[0])\n",
    "input_size_mom = n_final*4\n",
    "input_values_mom = mom_stdized.reshape(-1,input_size_mom)\n",
    "\n",
    "input_size_inv = len(inv_stdized[0])\n",
    "input_values_inv = inv_stdized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "backend.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_baseline(layers, input_size):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(layers[0], input_dim=(input_size)))\n",
    "    model.add(Activation(activations.tanh))\n",
    "        \n",
    "    for layer in range(1, len(layers)):\n",
    "        model.add(Dense(layers[layer]))\n",
    "        model.add(Activation(activations.tanh))\n",
    "\n",
    "    model.add(Dense(1))\n",
    "    \n",
    "    opt = Adam(learning_rate=1e-4, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    model.compile(optimizer=opt, loss='mean_squared_error', metrics=['mean_squared_error'])\n",
    "    return model\n",
    "\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=50, min_delta=1e-4, restore_best_weights=True, verbose=1),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-6, verbose=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_values_mominv = np.concatenate([input_values_mom,input_values_inv], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense (Dense)               (None, 20)                260       \n",
      "                                                                 \n",
      " activation (Activation)     (None, 20)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 40)                840       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 40)                0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 20)                820       \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 21        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,941\n",
      "Trainable params: 1,941\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-17 09:00:04.231214: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "model_mom = make_baseline([20,40,20],len(input_values_mom[0]))\n",
    "model_mom.summary()\n",
    "model_inv = make_baseline([20,40,20],len(input_values_inv[0]))\n",
    "model_mominv = make_baseline([20,40,20],len(input_values_mominv[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_mom_tr, in_mom_val, out_mom_tr, out_mom_val = train_test_split(input_values_mom, amp_stdized, test_size=0.2)\n",
    "in_inv_tr, in_inv_val, out_inv_tr, out_inv_val = train_test_split(input_values_inv, amp_stdized, test_size=0.2)\n",
    "in_mominv_tr, in_mominv_val, out_mominv_tr, out_mominv_val = train_test_split(input_values_mominv, amp_stdized, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time();\n",
    "\n",
    "history_mom = model_mom.fit(\n",
    "    in_mom_tr, out_mom_tr,\n",
    "    validation_data=(in_mom_val, out_mom_val),\n",
    "    batch_size=1024, epochs=2000, verbose=0,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print('trained in ',(et-st)/60.,' mins')\n",
    "\n",
    "print(f'Best case loss: {callbacks[0].best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time();\n",
    "\n",
    "history_inv = model_inv.fit(\n",
    "    in_inv_tr, out_inv_tr,\n",
    "    validation_data=(in_inv_val, out_inv_val),\n",
    "    batch_size=1024, epochs=2000, verbose=0,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print('trained in ',(et-st)/60.,' mins')\n",
    "\n",
    "print(f'Best case loss: {callbacks[0].best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time();\n",
    "\n",
    "history_mominv = model_mominv.fit(\n",
    "    in_mominv_tr, out_mominv_tr,\n",
    "    validation_data=(in_mominv_val, out_mominv_val),\n",
    "    batch_size=1024, epochs=2000, verbose=0,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "et = time.time()\n",
    "print('trained in ',(et-st)/60.,' mins')\n",
    "\n",
    "print(f'Best case loss: {callbacks[0].best}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the loss values from the training history.\n",
    "train_loss_mom = history_mom.history['loss']\n",
    "val_loss_mom = history_mom.history['val_loss']\n",
    "train_loss_inv = history_inv.history['loss']\n",
    "val_loss_inv = history_inv.history['val_loss']\n",
    "train_loss_mominv = history_mominv.history['loss']\n",
    "val_loss_mominv = history_mominv.history['val_loss']\n",
    "\n",
    "# Create a plot of the training and validation loss over epochs.\n",
    "epochs = range(1, len(train_loss_mom) + 1)\n",
    "plt.plot(epochs, train_loss_mom, label='Training Loss (mom. inputs)')\n",
    "plt.plot(epochs, val_loss_mom, label='Validation Loss (mom. inputs)')\n",
    "epochs = range(1, len(train_loss_inv) + 1)\n",
    "plt.plot(epochs, train_loss_inv, label='Training Loss (inv. inputs)')\n",
    "plt.plot(epochs, val_loss_inv, label='Validation Loss (inv. inputs)')\n",
    "epochs = range(1, len(train_loss_mominv) + 1)\n",
    "plt.plot(epochs, train_loss_mominv, label='Training Loss (mom.+inv. inputs)')\n",
    "plt.plot(epochs, val_loss_mominv, label='Validation Loss (mom.+inv. inputs)')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "#plt.ylim(0,0.01)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Test the model on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_test = mom_data[n_training_points:n_training_points+n_test_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mom_test_stdized = standardize(mom_test[:,2:,:],mom_ave, mom_std).reshape(-1,input_size_mom)\n",
    "\n",
    "inv_test = np.array([computeinvariants(mm) for mm in mom_test])\n",
    "inv_test_stdized = standardize(inv_test[:,1:], inv_ave, inv_std)\n",
    "\n",
    "mominv_test_stdized = np.concatenate([mom_test_stdized,inv_test_stdized], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_pred_mom = np.array(destandardize(\n",
    "    model_mom.predict(mom_test_stdized),\n",
    "    amp_ave, amp_std)).reshape(-1)\n",
    "\n",
    "amp_pred_inv = np.array(destandardize(\n",
    "    model_inv.predict(inv_test_stdized),\n",
    "    amp_ave, amp_std)).reshape(-1)\n",
    "\n",
    "amp_pred_mominv = np.array(destandardize(\n",
    "    model_mominv.predict(mominv_test_stdized),\n",
    "    amp_ave, amp_std)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_test = amp_data[n_training_points:n_training_points+n_test_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_mom = 2.*(amp_pred_mom-amp_test)/(amp_pred_mom+amp_test)\n",
    "acc_inv = 2.*(amp_pred_inv-amp_test)/(amp_pred_inv+amp_test)\n",
    "acc_mominv = 2.*(amp_pred_mominv-amp_test)/(amp_pred_mominv+amp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybins = np.histogram_bin_edges(acc_mom, bins=100, range=(-3,3))\n",
    "plt.hist(acc_mom, density=False, bins=mybins, alpha=0.5)\n",
    "plt.hist(acc_inv, density=False, bins=mybins, alpha=0.5)\n",
    "plt.hist(acc_mominv, density=False, bins=mybins, alpha=0.5)\n",
    "plt.xlim([-3,3])\n",
    "plt.xlabel('Accuracy = 2(Pred-Truth)/(Pred+Truth)|')\n",
    "plt.ylabel('Frequency');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logacc_mom = -np.log10(abs(acc_mominv))\n",
    "logacc_inv = -np.log10(abs(acc_inv))\n",
    "logacc_mominv = -np.log10(abs(acc_mominv))\n",
    "\n",
    "mybins = np.histogram_bin_edges(logacc_mom, bins=100, range=(-1,5))\n",
    "plt.hist(logacc_mom, density=False, bins=mybins, alpha=0.5, label='mom.')\n",
    "plt.hist(logacc_inv, density=False, bins=mybins, alpha=0.5, label='inv.')\n",
    "plt.hist(logacc_mominv, density=False, bins=mybins, alpha=0.5, label='mom.+inv.')\n",
    "plt.xlim([-1,5])\n",
    "plt.xlabel('Approx. Correct Digits = -Log10(|2(Pred-Truth)/(Pred+Truth)|)')\n",
    "plt.ylabel('Frequency');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Check the total cross-section #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now check the cross section which for a RAMBO phase-space is defined as (Leading Order)\n",
    "\n",
    "$\\sigma = 1/N_{trials} \\sum_p |A(p)|^2$\n",
    "\n",
    "where $N_{trials}$ is total number of random samples of momenta generated. Since JADE cuts have been applied the number of trials will not be equal to the number of phase-space points. This information was not given with the data set so we won't get the normalisation of the cross-section correct but the convergence cross-section can be tested using the mean value\n",
    "\n",
    "$\\hat{\\sigma} = \\langle |A(p)|^2 \\rangle_p$\n",
    "\n",
    "with the error estimate given by the standard deviation divided by the square root of the number of points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cross section for the test sample\n",
    "print(\"sigma_test = \",np.mean(amp_test),\"+/-\",np.std(amp_test)/np.sqrt(len(amp_test)))\n",
    "sigma_ref = np.mean(amp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total cross section using the three models\n",
    "print(\"sigma_pred_mom = \",np.mean(amp_pred_mom),\"+/-\",np.std(amp_pred_mom)/np.sqrt(len(amp_pred_mom)))\n",
    "print(\"sigma_pred_inv = \",np.mean(amp_pred_inv),\"+/-\",np.std(amp_pred_inv)/np.sqrt(len(amp_pred_inv)))\n",
    "print(\"sigma_pred_mominv = \",np.mean(amp_pred_mominv),\"+/-\",np.std(amp_pred_mominv)/np.sqrt(len(amp_pred_mominv)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now plot the cross-section cumilatively for the test set and models 1,2\n",
    "stepsize = 1000\n",
    "xs_NJ  = [[np.mean(amp_test[0:stepsize*i]), np.std(amp_test[0:stepsize*i])] for i in range(1,int(n_test_points/stepsize))]\n",
    "xs_NN1 = [[np.mean(amp_pred_mom[0:stepsize*i]), np.std(amp_pred_mom[0:stepsize*i])] for i in range(1,int(n_test_points/stepsize))]\n",
    "xs_NN2 = [[np.mean(amp_pred_inv[0:stepsize*i]), np.std(amp_pred_inv[0:stepsize*i])] for i in range(1,int(n_test_points/stepsize))]\n",
    "xs_NN3 = [[np.mean(amp_pred_mominv[0:stepsize*i]), np.std(amp_pred_mominv[0:stepsize*i])] for i in range(1,int(n_test_points/stepsize))]\n",
    "\n",
    "xs_NJ = np.array(xs_NJ)\n",
    "xs_NN1 = np.array(xs_NN1)\n",
    "xs_NN2 = np.array(xs_NN2)\n",
    "xs_NN3 = np.array(xs_NN3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This notation takes every 100 entries in the cumalative less so we don't plot so many points\n",
    "plotdata1 = xs_NJ[:,0]\n",
    "plotdata2 = xs_NN1[:,0]\n",
    "plotdata3 = xs_NN2[:,0]\n",
    "plotdata4 = xs_NN3[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we did the cumalative average correctly\n",
    "np.mean(amp_test)-plotdata1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(stepsize*np.array(range(len(plotdata1))), plotdata1, 'b-', lw=5, alpha=0.5, label='NJet')\n",
    "plt.plot(stepsize*np.array(range(len(plotdata2))), plotdata2, 'r-', label='mom.')\n",
    "plt.plot(stepsize*np.array(range(len(plotdata3))), plotdata3, 'g-', label='inv.')\n",
    "plt.plot(stepsize*np.array(range(len(plotdata4))), plotdata4, 'y-', label='mom.+inv.')\n",
    "plt.xlim([0,n_test_points])\n",
    "plt.ylim([sigma_ref*(1.-0.1),sigma_ref*(1.+0.1)])\n",
    "plt.ylabel('sigma')\n",
    "plt.xlabel('iteration');\n",
    "plt.title(r'e+e- -> '+str(jets)+'j at '+str(loop)+'-loop with JADE cut '+str(delta_cut))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also look at the size of the amplitude as a function of the accuracy of the predicitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.random.choice(len(amp_test), size=5000, replace=False)\n",
    "\n",
    "plt.scatter(logacc_mom[idx],np.log10(np.abs(amp_test[idx])),color='blue',alpha=0.7, label='mom.')\n",
    "plt.scatter(logacc_inv[idx],np.log10(np.abs(amp_test[idx])),color='orange',alpha=0.7, label='inv.')\n",
    "plt.scatter(logacc_mominv[idx],np.log10(np.abs(amp_test[idx])),color='lightgreen',alpha=0.7, label='mom.+inv.')\n",
    "plt.xlabel('Approx. Correct Digits')\n",
    "plt.ylabel('log10(|amplitude|)');\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "1. Keeping all the parameters unchanged, re-run the whole notebook. Does it makes sense that we don't get the same answer?\n",
    "2. Change the number of final state jets and the JADE cut 'delta_cut', does changing the network architecture improve the fit?\n",
    "3. Choose one-loop e+e- -> 3j and check the performance. Does it match your expectations?\n",
    "4. Does the activation function have an effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

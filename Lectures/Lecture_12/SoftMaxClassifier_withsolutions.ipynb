{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c93cbe8b",
   "metadata": {},
   "source": [
    "# An Example Soft-Max Classifier\n",
    "\n",
    "In this notebook we demonstrate how to design a classifier which divides a 2d region into three parts with non-linear boundaries.\n",
    "\n",
    "Example from https://cs231n.github.io/neural-networks-case-study/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d94d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3ab9a7",
   "metadata": {},
   "source": [
    "## 1. Data generation\n",
    "\n",
    "The first part of the code generates some data points in the (x,y) plane that lie around spiral lines starting at the origin. Three lines of points are generated with three colours (or 'classes')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce831104",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_points = 100 # number of points per class / number of points around each spiral\n",
    "D = 2 # dimensionality\n",
    "n_colours = 3 # number of classes / number of colours\n",
    "data_xy = np.zeros((n_points*n_colours,D)) # data matrix (each row = single example)\n",
    "data_col = np.zeros(n_points*n_colours, dtype='uint8') # class labels\n",
    "for j in range(n_colours):\n",
    "    ix = range(n_points*j,n_points*(j+1))\n",
    "    r = np.linspace(0.0,1,n_points) # radius\n",
    "    t = np.linspace(j*4,(j+1)*4,n_points) + np.random.randn(n_points)*0.2 # theta\n",
    "    data_xy[ix] = np.c_[r*np.sin(t), r*np.cos(t)]\n",
    "    data_col[ix] = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d557d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets visualize the data:\n",
    "plt.scatter(data_xy[:, 0], data_xy[:, 1], c=data_col, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697fac7c",
   "metadata": {},
   "source": [
    "# 2. Linear model\n",
    "\n",
    "Here we try to train a linear model built from scores, $z(X)$,\n",
    "\n",
    "  ${\\rm z}_j = X_{i} W_{ij}+b_{j}$\n",
    "\n",
    "where, $i=1,2$ (i.e. x,y coordinates), $j=1,n_{\\rm colours}$. The probability of this score is given using the sigmoid function:\n",
    "\n",
    "  $\\sigma(z_j) = \\frac{\\exp(-z_j)}{\\sum_k \\exp(-z_k)}$\n",
    "  \n",
    "repeated indices are understood to be summed over. By definition,\n",
    "\n",
    "  $\\sum_j \\sigma(z_j) = 1$\n",
    "  \n",
    "so $P(X) = \\sigma(z(X))$ outputs $n_{\\rm colours}$ values between 0 and 1 that sum to 1.\n",
    "\n",
    "The final prediction of the model would be the most likely of the possible colours:\n",
    "\n",
    "${\\rm colour}_{\\rm pred} = {\\rm argmax}(z(X))$\n",
    "\n",
    "which is a integer value between 1 and $n_{\\rm colours}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140b4092",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly (W - weights, b - biases)\n",
    "W = 0.01 * np.random.randn(D,n_colours)\n",
    "b = np.zeros((1,n_colours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7db050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute class scores for a linear classifier\n",
    "scores = np.dot(data_xy, W) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491f8f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_examples = data_xy.shape[0]\n",
    "# get unnormalized probabilities\n",
    "exp_scores = np.exp(scores)\n",
    "# normalize them for each example, list of \\sigma(z_i) defined above\n",
    "probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5635e786",
   "metadata": {},
   "source": [
    "We can now compute the -ve logarithm of the probability to use in the soft-max loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6f4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_logprobs = -np.log(probs[range(num_examples),data_col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5beab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the loss: average cross-entropy loss and regularization\n",
    "reg = 0.1\n",
    "\n",
    "data_loss = np.sum(correct_logprobs)/num_examples\n",
    "reg_loss = 0.5*reg*np.sum(W*W)\n",
    "loss = data_loss + reg_loss\n",
    "print(\"loss = \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93562b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dscores = probs\n",
    "dscores[range(num_examples),data_col] -= 1\n",
    "dscores /= num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41b2635e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = np.dot(data_xy.T, dscores)\n",
    "db = np.sum(dscores, axis=0, keepdims=True)\n",
    "dW += reg*W # don't forget the regularization gradient (data in this case has Gaussian noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4090d5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform a parameter update\n",
    "step_size = 0.1 # AKA learning rate\n",
    "W += -step_size * dW\n",
    "b += -step_size * db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505ba8f0",
   "metadata": {},
   "source": [
    "Now we iterate these steps to train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01091d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "W = 0.01 * np.random.randn(D,n_colours)\n",
    "b = np.zeros((1,n_colours))\n",
    "\n",
    "# some hyperparameters\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = data_xy.shape[0]\n",
    "for i in range(200):\n",
    "\n",
    "    # evaluate class scores, [N x n_colours]\n",
    "    scores = np.dot(data_xy, W) + b\n",
    "\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x n_colours]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),data_col])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 10 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),data_col] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters (W,b)\n",
    "    dW = np.dot(data_xy.T, dscores)\n",
    "    db = np.sum(dscores, axis=0, keepdims=True)\n",
    "\n",
    "    dW += reg*W # regularization gradient\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df56b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "scores = np.dot(data_xy, W) + b\n",
    "predicted_col = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_col == data_col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a338dd3e",
   "metadata": {},
   "source": [
    "# Exercise 1 #\n",
    "\n",
    "Make a plot of the prediction for the network for a grid in x and y and overset the orginal data set to show the region boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e6e4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.arange(-1,1,0.02)\n",
    "ypoints = xpoints\n",
    "xyinput = np.array([[i,j] for i in xpoints for j in ypoints])\n",
    "\n",
    "network_predicition = np.argmax(np.dot(xyinput, W) + b, axis=1)\n",
    "\n",
    "print(network_predicition)\n",
    "\n",
    "plt.scatter(xyinput[:,0], xyinput[:,1], c=network_predicition, cmap=plt.cm.Spectral, alpha=0.1)\n",
    "plt.scatter(data_xy[:, 0], data_xy[:, 1], c=data_col, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92895e4f",
   "metadata": {},
   "source": [
    "# 2. Non-linear model with hidden layer\n",
    "\n",
    "Now we add a hidden layer, include the computation of the gradient via backpropagation.\n",
    "\n",
    "The following lines demonstrate the steps required:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409a567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,n_colours)\n",
    "b2 = np.zeros((1,n_colours))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01cbd6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate class scores with a 2-layer Neural Network\n",
    "hidden_layer = np.maximum(0, np.dot(data_xy, W) + b) # note, ReLU activation\n",
    "scores = np.dot(hidden_layer, W2) + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a090bc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backpropate the gradient to the parameters\n",
    "# first backprop into parameters W2 and b2\n",
    "dW2 = np.dot(hidden_layer.T, dscores)\n",
    "db2 = np.sum(dscores, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d751cbf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dhidden = np.dot(dscores, W2.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c5699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backprop the ReLU non-linearity\n",
    "dhidden[hidden_layer <= 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253dc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally into W,b\n",
    "dW = np.dot(data_xy.T, dhidden)\n",
    "db = np.sum(dhidden, axis=0, keepdims=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf57103a",
   "metadata": {},
   "source": [
    "Now we can implement this in a loop over all data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0b8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize parameters randomly\n",
    "h = 100 # size of hidden layer\n",
    "W = 0.01 * np.random.randn(D,h)\n",
    "b = np.zeros((1,h))\n",
    "W2 = 0.01 * np.random.randn(h,n_colours)\n",
    "b2 = np.zeros((1,n_colours))\n",
    "\n",
    "# some hyperparameters for the gradient descent\n",
    "step_size = 1e-0\n",
    "reg = 1e-3 # regularization strength\n",
    "\n",
    "# gradient descent loop\n",
    "num_examples = data_xy.shape[0]\n",
    "for i in range(10000):\n",
    "\n",
    "    # evaluate class scores, [N x n_colours]\n",
    "    hidden_layer = np.maximum(0, np.dot(data_xy, W) + b) # note, ReLU activation\n",
    "    scores = np.dot(hidden_layer, W2) + b2\n",
    "\n",
    "    # compute the class probabilities\n",
    "    exp_scores = np.exp(scores)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True) # [N x K]\n",
    "\n",
    "    # compute the loss: average cross-entropy loss and regularization\n",
    "    correct_logprobs = -np.log(probs[range(num_examples),data_col])\n",
    "    data_loss = np.sum(correct_logprobs)/num_examples\n",
    "    reg_loss = 0.5*reg*np.sum(W*W) + 0.5*reg*np.sum(W2*W2)\n",
    "    loss = data_loss + reg_loss\n",
    "    if i % 1000 == 0:\n",
    "        print(\"iteration %d: loss %f\" % (i, loss))\n",
    "\n",
    "    # compute the gradient on scores\n",
    "    dscores = probs\n",
    "    dscores[range(num_examples),data_col] -= 1\n",
    "    dscores /= num_examples\n",
    "\n",
    "    # backpropate the gradient to the parameters\n",
    "    # first backprop into parameters W2 and b2\n",
    "    dW2 = np.dot(hidden_layer.T, dscores)\n",
    "    db2 = np.sum(dscores, axis=0, keepdims=True)\n",
    "    # next backprop into hidden layer\n",
    "    dhidden = np.dot(dscores, W2.T)\n",
    "    # backprop the ReLU non-linearity\n",
    "    dhidden[hidden_layer <= 0] = 0\n",
    "    # finally into W,b\n",
    "    dW = np.dot(data_xy.T, dhidden)\n",
    "    db = np.sum(dhidden, axis=0, keepdims=True)\n",
    "\n",
    "    # add regularization gradient contribution\n",
    "    dW2 += reg * W2\n",
    "    dW += reg * W\n",
    "\n",
    "    # perform a parameter update\n",
    "    W += -step_size * dW\n",
    "    b += -step_size * db\n",
    "    W2 += -step_size * dW2\n",
    "    b2 += -step_size * db2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb61c273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate training set accuracy\n",
    "hidden_layer = np.maximum(0, np.dot(data_xy, W) + b)\n",
    "scores = np.dot(hidden_layer, W2) + b2\n",
    "predicted_class = np.argmax(scores, axis=1)\n",
    "print('training accuracy: %.2f' % (np.mean(predicted_class == data_col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1f96a6",
   "metadata": {},
   "source": [
    "# Exercise 2 #\n",
    "\n",
    "Plot the prediction from the improved network and compare with the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9aea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "xpoints = np.arange(-1,1,0.02)\n",
    "ypoints = xpoints\n",
    "\n",
    "xyinput = np.array([[i,j] for i in xpoints for j in ypoints])\n",
    "\n",
    "network_predicition = np.argmax(np.dot(\n",
    "    np.maximum(0, np.dot(xyinput, W) + b)\n",
    "               ,W2)+b2, axis=1)\n",
    "\n",
    "print(network_predicition)\n",
    "\n",
    "plt.scatter(xyinput[:,0], xyinput[:,1], c=network_predicition, cmap=plt.cm.Spectral, alpha=0.1)\n",
    "plt.scatter(data_xy[:, 0], data_xy[:, 1], c=data_col, s=40, cmap=plt.cm.Spectral)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tutorial: SUSY Dataset - Introduction to Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source\n",
    "\n",
    "https://github.com/drckf/mlreview_notebooks/tree/master/jupyter_notebooks/notebooks\n",
    "\n",
    "as demonstrated in \"A high-bias, low-variance introduction to Machine Learning for physicists\" https://arxiv.org/pdf/1803.08823.pdf\n",
    "\n",
    "## Learning Goals ##\n",
    "This notebook will serve as an introduction to the logistic regression and will let us compare linear and non-linear models as well as different methods of regularization for the training of the model.\n",
    "\n",
    "## Overview ##\n",
    "Throughout, we will work with the [SUSY dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz). It is available from the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/datasets.html), which is a very comprehensive repository of useful datasets relevant to ML.\n",
    "\n",
    "Here is the description of the SUSY dataset we will be playing around with for this notebook:\n",
    ">The data has been produced using Monte Carlo simulations and contains events with two leptons (electrons or muons). In high energy physics experiments, such as the ATLAS and CMS detectors at the CERN LHC, one major hope is the discovery of new particles. To accomplish this task, physicists attempt to sift through data events and classify them as either a signal of some new physics process or particle, or instead a background event from understood Standard Model processes. Unfortunately we will never know for sure what underlying physical process happened (the only information to which we have access are the final state particles). However, we can attempt to define parts of phase space that will have a high percentage of signal events. Typically this is done by using a series of simple requirements on the kinematic quantities of the final state particles, for example having one or more leptons with large amounts of momentum that is transverse to the beam line ($p_{T}$). Here instead we will use logistic regression in order to attempt to find out the relative probability that an event is from a signal or a background event and rather than using the kinematic quantities of final state particles directly we will use the output of our logistic regression to define a part of phase space that is enriched in signal events. The dataset we are using has the value of 18 kinematic variables (\"features\") of the event. The first 8 features are direct measurements of final state particles, in this case the $p_{T}$, pseudo-rapidity ($\\eta$), and azimuthal angle ($\\phi$) of two leptons in the event and the amount of missing transverse momentum (MET) together with its azimuthal angle. The last ten features are functions of the first 8 features; these are high-level features derived by physicists to help discriminate between the two classes. You can think of them as physicists attempt to use non-linear functions to classify signal and background events and they have been developed with a lot of deep thinking on the part of physicist. There is however, an interest in using deep learning methods to obviate the need for physicists to manually develop such features. Benchmark results using Bayesian Decision Trees from a standard physics package and 5-layer neural networks and the dropout algorithm are presented in the original paper to compare the ability of deep-learning to bypass the need of using such high level features. We will also explore this topic in later notebooks. The dataset consists of 5 million events. For now 1,500,000 we will use for training the model and 50,000 examples will be used as a test set.\n",
    "\n",
    "This dataset comes from this interesting paper by the UCI group: <cite> P. Baldi, P. Sadowski, and D. Whiteson. \"Searching for Exotic Particles in High-energy Physics with Deep Learning.\" Nature Communications 5 (July 2, 2014)</cite>.\n",
    "\n",
    "## Categorical data/Classification Tasks ##\n",
    "\n",
    "So far, we have largely focused on supervised learning tasks such as linear regression, where the goal is to make predictions about continuous labels. Often, we are also interested in classification tasks -- where the goal is to assign samples to categories. The training data consists of a set of features and discrete labels. This type of data is called categorical data (the data comes in different categories). \n",
    "\n",
    "Initially, we will focus on a binary classification task. In the SUSY dataset, the goal is to decide whether a data point represents signal \"potential collision\"- labeled 1, or \"background\"(Standard Model processes which produce final states with similar constituents as SUSY processes) - labeled 0. This is done by looking at 18 features - the first 8 of which are \"low-level\" features that can be directly measured and the last 10 features are \"higher-order\" features constructed using physics intuition. In more detail:\n",
    ">The first column is the class label (1 for signal, 0 for background), followed by the 18 features (8 low-level features then 10 high-level features):: lepton 1 pT, lepton 1 eta, lepton 1 phi, lepton 2 pT, lepton 2 eta, lepton 2 phi, missing energy magnitude, missing energy phi, MET_rel, axial MET, M_R, M_TR_2, R, MT2, S_R, M_Delta_R, dPhi_r_b, cos(theta_r1)\n",
    "\n",
    "Our goal will be to use either the first 8 features or the full 18 features to predict whether an event is signal or background.\n",
    "\n",
    "## Logistic Regression ##\n",
    "\n",
    "\n",
    "One of the best understood and canonical methods for performing such a task is Logistic Regression. We will see that a deep understanding of Logistic regression will introduce us to many of the ideas and techniques at the forefront of modern Machine Learning. In Logistic regression, each set of features $\\mathbf{x}_i$ is associated with a category $C_i\\in\\{1,0\\}$, with $i=1\\ldots n$. It is helpful to re-define $\\mathbf{x}$ to be an extended vector $\\mathbf{x}\\rightarrow (1,\\mathbf{x})$ (which just accounts for an intercept, see Sec. VI of the review). Then, the Likelihood function for Logistic regression is given by the sigmoid function\n",
    "\n",
    "$$\n",
    "P(c_i=1)=1-P(c_i=0)= {1 \\over 1+ e^{-\\mathbf{w}\\cdot \\mathbf{x}_i}},\n",
    "$$\n",
    "\n",
    "where $\\mathbf{w}$ are the weights that define the logistic regression. Notice that this is just the Fermi function with $\\beta E=-\\mathbf{w}\\cdot \\mathbf{x}$.\n",
    "\n",
    "\n",
    "As in Sec. VI of the review, we will maximize the Log-likelihood of the observed data. Let us define the function\n",
    "$$\n",
    "f(a)={1 \\over 1+ e^{-a}},\n",
    "$$\n",
    "Notice that the derivative with respect to $a$ is given by\n",
    "$$\n",
    "{df \\over da}= f(1-f).\n",
    "$$\n",
    "\n",
    "Define $f_i \\equiv f(\\mathbf{w}\\cdot \\mathbf{x}_i)$. Then, the Likelihood of the data $\\{ \\mathbf{x}_i, C_i \\}$ is given by\n",
    "$$\n",
    "P(Data|\\mathbf{x})= \\prod_{i=1}^n f_i^{C_i}(1-f_i)^{1-C_i}\n",
    "$$\n",
    "and the log-likelihood is given by\n",
    "$$\n",
    "\\log{P(Data|\\mathbf{w})}= \\sum_{i=1}^n C_i \\log f_i + (1-C_i)\\log(1-f_i)\n",
    "$$\n",
    "\n",
    "The negative of the log-likelihood gives us the cross-entropy error function\n",
    "$$\n",
    "\\mathrm{Cross\\,Entropy}=E(\\mathbf{w})= -\\sum_{i=1}^n C_i \\log f_i + (1-C_i)\\log(1-f_i).\n",
    "$$\n",
    "\n",
    "Using the formula above notice that\n",
    "$$\n",
    "\\nabla E(\\mathbf{w})=\\sum_{i=1}^n (f_i-C_i)\\mathbf{x}_i.\n",
    "$$\n",
    "In other words, the gradient points in the sum of training example directions weighted by the difference between the true label and the probability of predicting that label.\n",
    "\n",
    "\n",
    "\n",
    "## Finding the MLE Estimate ##\n",
    "\n",
    "Notice the Maximum-Likelihood Estimation (MLE) is the same as minimizing the cross-entropy. There is no closed form expression for this. One strategy is to start with an arbitrary $\\mathbf{w}$ and then update our estimate based on our error function. In particular, we would like to nudge $\\mathbf{w}$ in the direction where the error is decreasing the fastest. This is the idea behind gradient descent. Furthermore, we can show that the cross-entropy error function used in logistic regression has a unique minimum. Thus, we can perform this procedure with relative ease (However, as a word of caution, note there is a generic instability in the MLE procedure for linearly separable data).\n",
    "\n",
    "Theoretically, one nice method for doing this is the <i> Newton-Raphson </i> method. In this method, we iteratively calculate the gradient \n",
    "$$\n",
    "\\mathbf{w}^{new} \\leftarrow \\mathbf{w}^{old} - \\mathbf{H}^{-1} \\nabla E(\\mathbf{w}),\n",
    "$$\n",
    "where $\\mathbf{H}$ is the Hessian matrix which is the second derivative of the energy function. For OLS linear regression, one can show that this procedure yields the right answer.\n",
    "\n",
    "More generally, there are a number of generalizations of this idea that have been proposed. We will refer to these kinds of methods as generalized gradient descent methods and discuss them extensively in what follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing the SUSY data set with Pandas\n",
    "\n",
    "In what follows, we use Pandas to import the first N_TRAIN examples as training data and the subsequent N_TEST examples as testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRAIN = 150000\n",
    "N_TEST = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data parsing is done!\n"
     ]
    }
   ],
   "source": [
    "# Importing the SUSY Data set\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gzip, io, requests\n",
    "import time\n",
    "\n",
    "#Comment the next line on to turn off warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "seed=12\n",
    "np.random.seed(seed)\n",
    "\n",
    "# To get the data set in a workable format, we can use two different methods.\n",
    "\n",
    "# Method 1 (advised for Jupyter Notebook)\n",
    "\n",
    "# Download the data SUSY.csv (about 2GB) one single time from UCI ML archive\n",
    "# and save it in the same directory as this notebook or in the appropriate drive folder\n",
    "\n",
    "# Origin data: https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz\n",
    "\n",
    "filename=\"SUSY.csv.gz\"\n",
    "columns=[\"signal\", \"lepton 1 pT\", \"lepton 1 eta\", \"lepton 1 phi\", \"lepton 2 pT\", \"lepton 2 eta\", \n",
    "         \"lepton 2 phi\", \"missing energy magnitude\", \"missing energy phi\", \"MET_rel\", \n",
    "          \"axial MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos(theta_r1)\"]\n",
    "\n",
    "# Load N_TRAIN rows as train data, N_TEST as test data\n",
    "df_train = pd.read_csv(filename,names=columns,nrows=N_TRAIN,engine='python')\n",
    "df_test = pd.read_csv(filename,names=columns,nrows=N_TEST, skiprows=N_TRAIN,engine='python')\n",
    "\n",
    "\n",
    "# Method 2 (advised for Google Colab)\n",
    "\n",
    "# download using url\n",
    "\n",
    "#url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00279/SUSY.csv.gz'\n",
    "#columns=[\"signal\", \"lepton 1 pT\", \"lepton 1 eta\", \"lepton 1 phi\", \"lepton 2 pT\", \"lepton 2 eta\", \n",
    "#         \"lepton 2 phi\", \"missing energy magnitude\", \"missing energy phi\", \"MET_rel\", \n",
    "#         \"axial MET\", \"M_R\", \"M_TR_2\", \"R\", \"MT2\", \"S_R\", \"M_Delta_R\", \"dPhi_r_b\", \"cos(theta_r1)\"]\n",
    "\n",
    "#web_response = requests.get(url, timeout=30, stream=True)\n",
    "#csv_gz_file = web_response.content # Content in bytes from requests.get\n",
    "                                     # See comments below why this is used.\n",
    "\n",
    "#f = io.BytesIO(csv_gz_file)\n",
    "#with gzip.GzipFile(fileobj=f) as filename:\n",
    "    # Passing a binary file to csv.reader works in PY2\n",
    "    #reader = csv.reader(fh)\n",
    "\n",
    "    # Load N_TRAIN rows as train data, N_TEST as test data\n",
    "#    df_train=pd.read_csv(filename,names=columns,nrows=N_TRAIN,engine='python')\n",
    "#    df_test=pd.read_csv(filename,names=columns,nrows=N_TEST, skiprows=N_TRAIN,engine='python')\n",
    "\n",
    "\n",
    "print(\"Data parsing is done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>signal</th>\n",
       "      <th>lepton 1 pT</th>\n",
       "      <th>lepton 1 eta</th>\n",
       "      <th>lepton 1 phi</th>\n",
       "      <th>lepton 2 pT</th>\n",
       "      <th>lepton 2 eta</th>\n",
       "      <th>lepton 2 phi</th>\n",
       "      <th>missing energy magnitude</th>\n",
       "      <th>missing energy phi</th>\n",
       "      <th>MET_rel</th>\n",
       "      <th>axial MET</th>\n",
       "      <th>M_R</th>\n",
       "      <th>M_TR_2</th>\n",
       "      <th>R</th>\n",
       "      <th>MT2</th>\n",
       "      <th>S_R</th>\n",
       "      <th>M_Delta_R</th>\n",
       "      <th>dPhi_r_b</th>\n",
       "      <th>cos(theta_r1)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.972861</td>\n",
       "      <td>0.653855</td>\n",
       "      <td>1.176225</td>\n",
       "      <td>1.157156</td>\n",
       "      <td>-1.739873</td>\n",
       "      <td>-0.874309</td>\n",
       "      <td>0.567765</td>\n",
       "      <td>-0.175000</td>\n",
       "      <td>0.810061</td>\n",
       "      <td>-0.252552</td>\n",
       "      <td>1.921887</td>\n",
       "      <td>0.889637</td>\n",
       "      <td>0.410772</td>\n",
       "      <td>1.145621</td>\n",
       "      <td>1.932632</td>\n",
       "      <td>0.994464</td>\n",
       "      <td>1.367815</td>\n",
       "      <td>0.040714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.667973</td>\n",
       "      <td>0.064191</td>\n",
       "      <td>-1.225171</td>\n",
       "      <td>0.506102</td>\n",
       "      <td>-0.338939</td>\n",
       "      <td>1.672543</td>\n",
       "      <td>3.475464</td>\n",
       "      <td>-1.219136</td>\n",
       "      <td>0.012955</td>\n",
       "      <td>3.775174</td>\n",
       "      <td>1.045977</td>\n",
       "      <td>0.568051</td>\n",
       "      <td>0.481928</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.448410</td>\n",
       "      <td>0.205356</td>\n",
       "      <td>1.321893</td>\n",
       "      <td>0.377584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.444840</td>\n",
       "      <td>-0.134298</td>\n",
       "      <td>-0.709972</td>\n",
       "      <td>0.451719</td>\n",
       "      <td>-1.613871</td>\n",
       "      <td>-0.768661</td>\n",
       "      <td>1.219918</td>\n",
       "      <td>0.504026</td>\n",
       "      <td>1.831248</td>\n",
       "      <td>-0.431385</td>\n",
       "      <td>0.526283</td>\n",
       "      <td>0.941514</td>\n",
       "      <td>1.587535</td>\n",
       "      <td>2.024308</td>\n",
       "      <td>0.603498</td>\n",
       "      <td>1.562374</td>\n",
       "      <td>1.135454</td>\n",
       "      <td>0.180910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.381256</td>\n",
       "      <td>-0.976145</td>\n",
       "      <td>0.693152</td>\n",
       "      <td>0.448959</td>\n",
       "      <td>0.891753</td>\n",
       "      <td>-0.677328</td>\n",
       "      <td>2.033060</td>\n",
       "      <td>1.533041</td>\n",
       "      <td>3.046260</td>\n",
       "      <td>-1.005285</td>\n",
       "      <td>0.569386</td>\n",
       "      <td>1.015211</td>\n",
       "      <td>1.582217</td>\n",
       "      <td>1.551914</td>\n",
       "      <td>0.761215</td>\n",
       "      <td>1.715464</td>\n",
       "      <td>1.492257</td>\n",
       "      <td>0.090719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.309996</td>\n",
       "      <td>-0.690089</td>\n",
       "      <td>-0.676259</td>\n",
       "      <td>1.589283</td>\n",
       "      <td>-0.693326</td>\n",
       "      <td>0.622907</td>\n",
       "      <td>1.087562</td>\n",
       "      <td>-0.381742</td>\n",
       "      <td>0.589204</td>\n",
       "      <td>1.365479</td>\n",
       "      <td>1.179295</td>\n",
       "      <td>0.968218</td>\n",
       "      <td>0.728563</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.083158</td>\n",
       "      <td>0.043429</td>\n",
       "      <td>1.154854</td>\n",
       "      <td>0.094859</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   signal  lepton 1 pT  lepton 1 eta  lepton 1 phi  lepton 2 pT  lepton 2 eta  \\\n",
       "0     0.0     0.972861      0.653855      1.176225     1.157156     -1.739873   \n",
       "1     1.0     1.667973      0.064191     -1.225171     0.506102     -0.338939   \n",
       "2     1.0     0.444840     -0.134298     -0.709972     0.451719     -1.613871   \n",
       "3     1.0     0.381256     -0.976145      0.693152     0.448959      0.891753   \n",
       "4     1.0     1.309996     -0.690089     -0.676259     1.589283     -0.693326   \n",
       "\n",
       "   lepton 2 phi  missing energy magnitude  missing energy phi   MET_rel  \\\n",
       "0     -0.874309                  0.567765           -0.175000  0.810061   \n",
       "1      1.672543                  3.475464           -1.219136  0.012955   \n",
       "2     -0.768661                  1.219918            0.504026  1.831248   \n",
       "3     -0.677328                  2.033060            1.533041  3.046260   \n",
       "4      0.622907                  1.087562           -0.381742  0.589204   \n",
       "\n",
       "   axial MET       M_R    M_TR_2         R       MT2       S_R  M_Delta_R  \\\n",
       "0  -0.252552  1.921887  0.889637  0.410772  1.145621  1.932632   0.994464   \n",
       "1   3.775174  1.045977  0.568051  0.481928  0.000000  0.448410   0.205356   \n",
       "2  -0.431385  0.526283  0.941514  1.587535  2.024308  0.603498   1.562374   \n",
       "3  -1.005285  0.569386  1.015211  1.582217  1.551914  0.761215   1.715464   \n",
       "4   1.365479  1.179295  0.968218  0.728563  0.000000  1.083158   0.043429   \n",
       "\n",
       "   dPhi_r_b  cos(theta_r1)  \n",
       "0  1.367815       0.040714  \n",
       "1  1.321893       0.377584  \n",
       "2  1.135454       0.180910  \n",
       "3  1.492257       0.090719  \n",
       "4  1.154854       0.094859  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show a snippet of the training data that we have just downloaded\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression with scikit-learn packages\n",
    "\n",
    "Let us define some functions to do the actual training for us and will let them take the number of features as an input parameter.\n",
    "\n",
    "We'll be mainly using:\n",
    "\n",
    "1. Multi-layer Perceptron classifier. (comes with L2 regularization)\n",
    "2. Linear classifiers with SGD (stochastic gradient descent) training. (comes with L1 and L2 regularization)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n",
    "\n",
    "in particular:\n",
    "\n",
    "alpha (float), default=0.0001\n",
    "Strength of the L2 regularization term. The L2 regularization term is divided by the sample size when added to the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "#import ml_style as style #optional styling sheet\n",
    "#mpl.rcParams.update(style.style) #optional styling sheet\n",
    "\n",
    "def getTrainData(nVar):\n",
    "    designMatrix = df_train.iloc[:,1:nVar+1].values\n",
    "    #now the signal\n",
    "    labels = df_train['signal'].values # labels (0 or 1)\n",
    "    return (designMatrix,labels)\n",
    "\n",
    "def getTestData(nVar):\n",
    "    designMatrix = df_test.iloc[:,1:nVar+1].values\n",
    "    #now the signal\n",
    "    labels = df_test['signal'].values\n",
    "    return (designMatrix,labels)\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "\n",
    "def runSciKitNNRegression(nVar, alpha):\n",
    "    X_train, y_train = getTrainData(nVar)\n",
    "    X_test, y_test = getTestData(nVar)\n",
    "    clf = MLPClassifier(alpha=alpha,max_iter=5).fit(X_train,y_train)\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('MLP+L2: Accuracy on test data with alpha %.2E : %.3f' %(alpha,clf.score(X_test,y_test)) )\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    \n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs[:,1])\n",
    "    print(\"AUC  = \", roc_auc_score(y_test, probs[:,1]))\n",
    "    return (probs, fpr, tpr)\n",
    "\n",
    "def runSciKitRegressionL1(nVar,alpha):\n",
    "    X_train, y_train = getTrainData(nVar)\n",
    "    X_test, y_test = getTestData(nVar)\n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"l1\",alpha=alpha,max_iter=5,tol=None)\n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('SGD+L1: Accuracy on test data with alpha %.2E : %.3f' %(alpha,clf.score(X_test,y_test)) )\n",
    "    probs = clf.predict_proba(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs[:,1])\n",
    "    print(\"AUC  = \", roc_auc_score(y_test, probs[:,1]))\n",
    "    return (probs, fpr, tpr)\n",
    "\n",
    "def runSciKitRegressionL2(nVar, alpha):\n",
    "    X_train, y_train = getTrainData(nVar)\n",
    "    X_test, y_test = getTestData(nVar)\n",
    "    clf = SGDClassifier(loss=\"log\", penalty=\"l2\",alpha=alpha,max_iter=5,tol=None)\n",
    "    clf.fit(X_train,y_train.ravel())\n",
    "    predictions = clf.predict(X_test)\n",
    "    print('SGD+L2: Accuracy on test data with alpha %.2E : %.3f' %(alpha,clf.score(X_test,y_test)) )\n",
    "    probs = clf.predict_proba(X_test)\n",
    "\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs[:,1])\n",
    "    print(\"AUC  = \", roc_auc_score(y_test, probs[:,1]))\n",
    "    return (probs, fpr, tpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definitions\n",
    "\n",
    "## 1. What is AUC - ROC Curve?\n",
    "\n",
    "AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is.\n",
    "\n",
    "More information here: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5\n",
    "\n",
    "## 2. Regularisation\n",
    "\n",
    "A regression model that uses L1 regularization technique is called Lasso Regression and model which uses L2 is called Ridge Regression.\n",
    "\n",
    "### 2a. What is L1 regularisation?\n",
    "\n",
    "Lasso (L1) shrinks the less important feature’s coefficient to zero thus, **removing some features altogether**.\n",
    "This in turn **reduces the model complexity**, making our model simpler. **A simpler model can reduce the chances of overfitting**.\n",
    "L1 regularisation is defined as \n",
    "$$ ||w||_1 = (|w_1| + |w_2| + ...) $$\n",
    "\n",
    "### 2b. What is L2 regularisation?\n",
    "\n",
    "Overfitting is a phenomenon that occurs when a machine learning or statistics model is tailored to a particular dataset and is unable to generalise to other datasets. This usually happens in complex models, like deep neural networks.\n",
    "Regularisation is a process of introducing additional information in order to **prevent overfitting**. \n",
    "\n",
    "A network with large network weights can be a sign of an unstable network where small changes in the input can lead to large changes in the output. This can be a sign that the network has overfit the training dataset and will likely perform poorly when making predictions on new data.\n",
    "\n",
    "A solution to this problem is to update the learning algorithm to encourage the network to keep the weights small. This is called weight regularization and it can be used as a general technique to reduce overfitting of the training dataset and improve the generalization of the model.\n",
    "\n",
    "L2 regularisation takes its name from `L2 norm of a vector w` defined as:\n",
    "$$ ||w||_2 = (w_1^2 + w_2^2 + ...)^{1/2} $$\n",
    "\n",
    "**L2 regularization shrinks the size of the parameters towards zero**.\n",
    "\n",
    "More information here: https://towardsdatascience.com/intuitions-on-l1-and-l2-regularisation-235f2db4c261#f810\n",
    "\n",
    "---\n",
    "\n",
    "When we say a loss function is L1 or L2 regularized, that means that the above terms are added to the loss function in function of the parameters to be optimized:\n",
    "$$ {\\rm Loss}_{\\rm regularized}(w) = {\\rm Loss}(w) + \\alpha*||w||_{1,2}$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a Multi-layer Perceptron classifier (8 features)\n",
    "\n",
    "We now will run logistic regression using a Multi-layer Perceptron classifier on the SUSY data for both the simple features (first 8 features) and the full feature space. First we will start with only the low-level features. We will also investigate the use of [Ridge Regression](https://ncss-wpengine.netdna-ssl.com/wp-content/themes/ncss/pdf/Procedures/NCSS/Ridge_Regression.pdf) by testing the results as as function of the regularization parameter $\\alpha$. \n",
    "\n",
    "Now let's run using the first 8 variables only (the low-level features). Note: this will take several minutes !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP+L2: Accuracy on test data with alpha 1.00E-03 : 0.787\n",
      "AUC  =  0.8574656797229749\n"
     ]
    }
   ],
   "source": [
    "alpha = 10**-3\n",
    "\n",
    "X_train, y_train = getTrainData(8)\n",
    "X_test, y_test = getTestData(8)\n",
    "clf = MLPClassifier(alpha=alpha,max_iter=5).fit(X_train,y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "print('MLP+L2: Accuracy on test data with alpha %.2E : %.3f' %(alpha,clf.score(X_test,y_test)) )\n",
    "probs = clf.predict_proba(X_test)\n",
    "    \n",
    "fpr, tpr, thresholds2 = roc_curve(y_test, probs[:,1])\n",
    "print(\"AUC  = \", roc_auc_score(y_test, probs[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make the ROC curve by hand one time to check everything is as expected\n",
    "\n",
    "TT = np.linspace(0.,1., num=100)\n",
    "TP = []\n",
    "TN = []\n",
    "FP = []\n",
    "FN = []\n",
    "\n",
    "for tt in TT:\n",
    "    positive_preds = np.array([int(pp>tt) for pp in probs[:,1]])\n",
    "\n",
    "    test_classification = np.transpose([positive_preds,y_test]).tolist()\n",
    "    TP.append(test_classification.count([1.,1.]))\n",
    "    TN.append(test_classification.count([0.,0.]))\n",
    "    FP.append(test_classification.count([1.,0.]))\n",
    "    FN.append(test_classification.count([0.,1.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fc2b1fa9630>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       "  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Created with matplotlib (https://matplotlib.org/) -->\n",
       "<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 381.65 248.518125\" width=\"381.65pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       " <metadata>\n",
       "  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n",
       "   <cc:Work>\n",
       "    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n",
       "    <dc:date>2023-11-13T11:20:09.138526</dc:date>\n",
       "    <dc:format>image/svg+xml</dc:format>\n",
       "    <dc:creator>\n",
       "     <cc:Agent>\n",
       "      <dc:title>Matplotlib v3.3.4, https://matplotlib.org/</dc:title>\n",
       "     </cc:Agent>\n",
       "    </dc:creator>\n",
       "   </cc:Work>\n",
       "  </rdf:RDF>\n",
       " </metadata>\n",
       " <defs>\n",
       "  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n",
       " </defs>\n",
       " <g id=\"figure_1\">\n",
       "  <g id=\"patch_1\">\n",
       "   <path d=\"M -0 248.518125 \n",
       "L 381.65 248.518125 \n",
       "L 381.65 0 \n",
       "L -0 0 \n",
       "z\n",
       "\" style=\"fill:none;\"/>\n",
       "  </g>\n",
       "  <g id=\"axes_1\">\n",
       "   <g id=\"patch_2\">\n",
       "    <path d=\"M 39.65 224.64 \n",
       "L 374.45 224.64 \n",
       "L 374.45 7.2 \n",
       "L 39.65 7.2 \n",
       "z\n",
       "\" style=\"fill:#ffffff;\"/>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_1\">\n",
       "    <g id=\"xtick_1\">\n",
       "     <g id=\"line2d_1\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L 0 3.5 \n",
       "\" id=\"md7399fe2ad\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"54.868182\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_1\">\n",
       "      <!-- 0.0 -->\n",
       "      <g transform=\"translate(46.916619 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 66.40625 \n",
       "Q 24.171875 66.40625 20.328125 58.90625 \n",
       "Q 16.5 51.421875 16.5 36.375 \n",
       "Q 16.5 21.390625 20.328125 13.890625 \n",
       "Q 24.171875 6.390625 31.78125 6.390625 \n",
       "Q 39.453125 6.390625 43.28125 13.890625 \n",
       "Q 47.125 21.390625 47.125 36.375 \n",
       "Q 47.125 51.421875 43.28125 58.90625 \n",
       "Q 39.453125 66.40625 31.78125 66.40625 \n",
       "z\n",
       "M 31.78125 74.21875 \n",
       "Q 44.046875 74.21875 50.515625 64.515625 \n",
       "Q 56.984375 54.828125 56.984375 36.375 \n",
       "Q 56.984375 17.96875 50.515625 8.265625 \n",
       "Q 44.046875 -1.421875 31.78125 -1.421875 \n",
       "Q 19.53125 -1.421875 13.0625 8.265625 \n",
       "Q 6.59375 17.96875 6.59375 36.375 \n",
       "Q 6.59375 54.828125 13.0625 64.515625 \n",
       "Q 19.53125 74.21875 31.78125 74.21875 \n",
       "z\n",
       "\" id=\"DejaVuSans-48\"/>\n",
       "        <path d=\"M 10.6875 12.40625 \n",
       "L 21 12.40625 \n",
       "L 21 0 \n",
       "L 10.6875 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-46\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_2\">\n",
       "     <g id=\"line2d_2\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"115.740909\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_2\">\n",
       "      <!-- 0.2 -->\n",
       "      <g transform=\"translate(107.789347 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 19.1875 8.296875 \n",
       "L 53.609375 8.296875 \n",
       "L 53.609375 0 \n",
       "L 7.328125 0 \n",
       "L 7.328125 8.296875 \n",
       "Q 12.9375 14.109375 22.625 23.890625 \n",
       "Q 32.328125 33.6875 34.8125 36.53125 \n",
       "Q 39.546875 41.84375 41.421875 45.53125 \n",
       "Q 43.3125 49.21875 43.3125 52.78125 \n",
       "Q 43.3125 58.59375 39.234375 62.25 \n",
       "Q 35.15625 65.921875 28.609375 65.921875 \n",
       "Q 23.96875 65.921875 18.8125 64.3125 \n",
       "Q 13.671875 62.703125 7.8125 59.421875 \n",
       "L 7.8125 69.390625 \n",
       "Q 13.765625 71.78125 18.9375 73 \n",
       "Q 24.125 74.21875 28.421875 74.21875 \n",
       "Q 39.75 74.21875 46.484375 68.546875 \n",
       "Q 53.21875 62.890625 53.21875 53.421875 \n",
       "Q 53.21875 48.921875 51.53125 44.890625 \n",
       "Q 49.859375 40.875 45.40625 35.40625 \n",
       "Q 44.1875 33.984375 37.640625 27.21875 \n",
       "Q 31.109375 20.453125 19.1875 8.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-50\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-50\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_3\">\n",
       "     <g id=\"line2d_3\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"176.613636\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_3\">\n",
       "      <!-- 0.4 -->\n",
       "      <g transform=\"translate(168.662074 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 37.796875 64.3125 \n",
       "L 12.890625 25.390625 \n",
       "L 37.796875 25.390625 \n",
       "z\n",
       "M 35.203125 72.90625 \n",
       "L 47.609375 72.90625 \n",
       "L 47.609375 25.390625 \n",
       "L 58.015625 25.390625 \n",
       "L 58.015625 17.1875 \n",
       "L 47.609375 17.1875 \n",
       "L 47.609375 0 \n",
       "L 37.796875 0 \n",
       "L 37.796875 17.1875 \n",
       "L 4.890625 17.1875 \n",
       "L 4.890625 26.703125 \n",
       "z\n",
       "\" id=\"DejaVuSans-52\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-52\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_4\">\n",
       "     <g id=\"line2d_4\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"237.486364\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_4\">\n",
       "      <!-- 0.6 -->\n",
       "      <g transform=\"translate(229.534801 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 33.015625 40.375 \n",
       "Q 26.375 40.375 22.484375 35.828125 \n",
       "Q 18.609375 31.296875 18.609375 23.390625 \n",
       "Q 18.609375 15.53125 22.484375 10.953125 \n",
       "Q 26.375 6.390625 33.015625 6.390625 \n",
       "Q 39.65625 6.390625 43.53125 10.953125 \n",
       "Q 47.40625 15.53125 47.40625 23.390625 \n",
       "Q 47.40625 31.296875 43.53125 35.828125 \n",
       "Q 39.65625 40.375 33.015625 40.375 \n",
       "z\n",
       "M 52.59375 71.296875 \n",
       "L 52.59375 62.3125 \n",
       "Q 48.875 64.0625 45.09375 64.984375 \n",
       "Q 41.3125 65.921875 37.59375 65.921875 \n",
       "Q 27.828125 65.921875 22.671875 59.328125 \n",
       "Q 17.53125 52.734375 16.796875 39.40625 \n",
       "Q 19.671875 43.65625 24.015625 45.921875 \n",
       "Q 28.375 48.1875 33.59375 48.1875 \n",
       "Q 44.578125 48.1875 50.953125 41.515625 \n",
       "Q 57.328125 34.859375 57.328125 23.390625 \n",
       "Q 57.328125 12.15625 50.6875 5.359375 \n",
       "Q 44.046875 -1.421875 33.015625 -1.421875 \n",
       "Q 20.359375 -1.421875 13.671875 8.265625 \n",
       "Q 6.984375 17.96875 6.984375 36.375 \n",
       "Q 6.984375 53.65625 15.1875 63.9375 \n",
       "Q 23.390625 74.21875 37.203125 74.21875 \n",
       "Q 40.921875 74.21875 44.703125 73.484375 \n",
       "Q 48.484375 72.75 52.59375 71.296875 \n",
       "z\n",
       "\" id=\"DejaVuSans-54\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-54\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_5\">\n",
       "     <g id=\"line2d_5\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"298.359091\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_5\">\n",
       "      <!-- 0.8 -->\n",
       "      <g transform=\"translate(290.407528 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 31.78125 34.625 \n",
       "Q 24.75 34.625 20.71875 30.859375 \n",
       "Q 16.703125 27.09375 16.703125 20.515625 \n",
       "Q 16.703125 13.921875 20.71875 10.15625 \n",
       "Q 24.75 6.390625 31.78125 6.390625 \n",
       "Q 38.8125 6.390625 42.859375 10.171875 \n",
       "Q 46.921875 13.96875 46.921875 20.515625 \n",
       "Q 46.921875 27.09375 42.890625 30.859375 \n",
       "Q 38.875 34.625 31.78125 34.625 \n",
       "z\n",
       "M 21.921875 38.8125 \n",
       "Q 15.578125 40.375 12.03125 44.71875 \n",
       "Q 8.5 49.078125 8.5 55.328125 \n",
       "Q 8.5 64.0625 14.71875 69.140625 \n",
       "Q 20.953125 74.21875 31.78125 74.21875 \n",
       "Q 42.671875 74.21875 48.875 69.140625 \n",
       "Q 55.078125 64.0625 55.078125 55.328125 \n",
       "Q 55.078125 49.078125 51.53125 44.71875 \n",
       "Q 48 40.375 41.703125 38.8125 \n",
       "Q 48.828125 37.15625 52.796875 32.3125 \n",
       "Q 56.78125 27.484375 56.78125 20.515625 \n",
       "Q 56.78125 9.90625 50.3125 4.234375 \n",
       "Q 43.84375 -1.421875 31.78125 -1.421875 \n",
       "Q 19.734375 -1.421875 13.25 4.234375 \n",
       "Q 6.78125 9.90625 6.78125 20.515625 \n",
       "Q 6.78125 27.484375 10.78125 32.3125 \n",
       "Q 14.796875 37.15625 21.921875 38.8125 \n",
       "z\n",
       "M 18.3125 54.390625 \n",
       "Q 18.3125 48.734375 21.84375 45.5625 \n",
       "Q 25.390625 42.390625 31.78125 42.390625 \n",
       "Q 38.140625 42.390625 41.71875 45.5625 \n",
       "Q 45.3125 48.734375 45.3125 54.390625 \n",
       "Q 45.3125 60.0625 41.71875 63.234375 \n",
       "Q 38.140625 66.40625 31.78125 66.40625 \n",
       "Q 25.390625 66.40625 21.84375 63.234375 \n",
       "Q 18.3125 60.0625 18.3125 54.390625 \n",
       "z\n",
       "\" id=\"DejaVuSans-56\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-56\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"xtick_6\">\n",
       "     <g id=\"line2d_6\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.231818\" xlink:href=\"#md7399fe2ad\" y=\"224.64\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_6\">\n",
       "      <!-- 1.0 -->\n",
       "      <g transform=\"translate(351.280256 239.238437)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 12.40625 8.296875 \n",
       "L 28.515625 8.296875 \n",
       "L 28.515625 63.921875 \n",
       "L 10.984375 60.40625 \n",
       "L 10.984375 69.390625 \n",
       "L 28.421875 72.90625 \n",
       "L 38.28125 72.90625 \n",
       "L 38.28125 8.296875 \n",
       "L 54.390625 8.296875 \n",
       "L 54.390625 0 \n",
       "L 12.40625 0 \n",
       "z\n",
       "\" id=\"DejaVuSans-49\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-46\"/>\n",
       "       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"matplotlib.axis_2\">\n",
       "    <g id=\"ytick_1\">\n",
       "     <g id=\"line2d_7\">\n",
       "      <defs>\n",
       "       <path d=\"M 0 0 \n",
       "L -3.5 0 \n",
       "\" id=\"md74255493b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n",
       "      </defs>\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#md74255493b\" y=\"214.756364\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_7\">\n",
       "      <!-- 0 -->\n",
       "      <g transform=\"translate(26.2875 218.555582)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_2\">\n",
       "     <g id=\"line2d_8\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#md74255493b\" y=\"171.520859\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_8\">\n",
       "      <!-- 500 -->\n",
       "      <g transform=\"translate(13.5625 175.320078)scale(0.1 -0.1)\">\n",
       "       <defs>\n",
       "        <path d=\"M 10.796875 72.90625 \n",
       "L 49.515625 72.90625 \n",
       "L 49.515625 64.59375 \n",
       "L 19.828125 64.59375 \n",
       "L 19.828125 46.734375 \n",
       "Q 21.96875 47.46875 24.109375 47.828125 \n",
       "Q 26.265625 48.1875 28.421875 48.1875 \n",
       "Q 40.625 48.1875 47.75 41.5 \n",
       "Q 54.890625 34.8125 54.890625 23.390625 \n",
       "Q 54.890625 11.625 47.5625 5.09375 \n",
       "Q 40.234375 -1.421875 26.90625 -1.421875 \n",
       "Q 22.3125 -1.421875 17.546875 -0.640625 \n",
       "Q 12.796875 0.140625 7.71875 1.703125 \n",
       "L 7.71875 11.625 \n",
       "Q 12.109375 9.234375 16.796875 8.0625 \n",
       "Q 21.484375 6.890625 26.703125 6.890625 \n",
       "Q 35.15625 6.890625 40.078125 11.328125 \n",
       "Q 45.015625 15.765625 45.015625 23.390625 \n",
       "Q 45.015625 31 40.078125 35.4375 \n",
       "Q 35.15625 39.890625 26.703125 39.890625 \n",
       "Q 22.75 39.890625 18.8125 39.015625 \n",
       "Q 14.890625 38.140625 10.796875 36.28125 \n",
       "z\n",
       "\" id=\"DejaVuSans-53\"/>\n",
       "       </defs>\n",
       "       <use xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_3\">\n",
       "     <g id=\"line2d_9\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#md74255493b\" y=\"128.285354\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_9\">\n",
       "      <!-- 1000 -->\n",
       "      <g transform=\"translate(7.2 132.084573)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_4\">\n",
       "     <g id=\"line2d_10\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#md74255493b\" y=\"85.04985\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_10\">\n",
       "      <!-- 1500 -->\n",
       "      <g transform=\"translate(7.2 88.849068)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-49\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-53\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "    <g id=\"ytick_5\">\n",
       "     <g id=\"line2d_11\">\n",
       "      <g>\n",
       "       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"39.65\" xlink:href=\"#md74255493b\" y=\"41.814345\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "     <g id=\"text_11\">\n",
       "      <!-- 2000 -->\n",
       "      <g transform=\"translate(7.2 45.613564)scale(0.1 -0.1)\">\n",
       "       <use xlink:href=\"#DejaVuSans-50\"/>\n",
       "       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-48\"/>\n",
       "      </g>\n",
       "     </g>\n",
       "    </g>\n",
       "   </g>\n",
       "   <g id=\"line2d_12\">\n",
       "    <path clip-path=\"url(#p6e7cecef2a)\" d=\"M 54.868182 17.083636 \n",
       "L 57.942562 17.083636 \n",
       "L 61.016942 17.083636 \n",
       "L 64.091322 17.083636 \n",
       "L 67.165702 17.083636 \n",
       "L 70.240083 17.42952 \n",
       "L 73.314463 17.688933 \n",
       "L 76.388843 18.034817 \n",
       "L 79.463223 18.640115 \n",
       "L 82.537603 19.591296 \n",
       "L 85.611983 20.196593 \n",
       "L 88.686364 21.061303 \n",
       "L 91.760744 21.753071 \n",
       "L 94.835124 22.963665 \n",
       "L 97.909504 23.914846 \n",
       "L 100.983884 24.693085 \n",
       "L 104.058264 25.903679 \n",
       "L 107.132645 27.806042 \n",
       "L 110.207025 29.448991 \n",
       "L 113.281405 30.573114 \n",
       "L 116.355785 32.389005 \n",
       "L 119.430165 33.167244 \n",
       "L 122.504545 34.464309 \n",
       "L 125.578926 35.674903 \n",
       "L 128.653306 37.05844 \n",
       "L 131.727686 38.528447 \n",
       "L 134.802066 39.479628 \n",
       "L 137.876446 40.51728 \n",
       "L 140.950826 41.900816 \n",
       "L 144.025207 43.024939 \n",
       "L 147.099587 44.581417 \n",
       "L 150.173967 46.310838 \n",
       "L 153.248347 47.694374 \n",
       "L 156.322727 49.07791 \n",
       "L 159.397107 50.893801 \n",
       "L 162.471488 52.104395 \n",
       "L 165.545868 53.142047 \n",
       "L 168.620248 54.352641 \n",
       "L 171.694628 55.303822 \n",
       "L 174.769008 56.860301 \n",
       "L 177.843388 57.811482 \n",
       "L 180.917769 59.454431 \n",
       "L 183.992149 60.578554 \n",
       "L 187.066529 62.048561 \n",
       "L 190.140909 63.345626 \n",
       "L 193.215289 64.815634 \n",
       "L 196.289669 65.939757 \n",
       "L 199.36405 67.496235 \n",
       "L 202.43843 70.003894 \n",
       "L 205.51281 72.079198 \n",
       "L 208.58719 73.462734 \n",
       "L 211.66157 74.932742 \n",
       "L 214.73595 76.402749 \n",
       "L 217.810331 77.613343 \n",
       "L 220.884711 79.08335 \n",
       "L 223.959091 80.466886 \n",
       "L 227.033471 81.67748 \n",
       "L 230.107851 82.888074 \n",
       "L 233.182231 84.18514 \n",
       "L 236.256612 85.741618 \n",
       "L 239.330992 87.471038 \n",
       "L 242.405372 88.681632 \n",
       "L 245.479752 89.978697 \n",
       "L 248.554132 91.708117 \n",
       "L 251.628512 92.74577 \n",
       "L 254.702893 94.388719 \n",
       "L 257.777273 96.464023 \n",
       "L 260.851653 98.279914 \n",
       "L 263.926033 100.355218 \n",
       "L 267.000413 102.603465 \n",
       "L 270.074793 104.851711 \n",
       "L 273.149174 106.49466 \n",
       "L 276.223554 108.22408 \n",
       "L 279.297934 110.645268 \n",
       "L 282.372314 112.634102 \n",
       "L 285.446694 114.19058 \n",
       "L 288.521074 116.525297 \n",
       "L 291.595455 118.51413 \n",
       "L 294.669835 120.502963 \n",
       "L 297.744215 121.800029 \n",
       "L 300.818595 123.961804 \n",
       "L 303.892975 125.950637 \n",
       "L 306.967355 128.977122 \n",
       "L 310.041736 131.398311 \n",
       "L 313.116116 134.77068 \n",
       "L 316.190496 136.932455 \n",
       "L 319.264876 139.353644 \n",
       "L 322.339256 141.688361 \n",
       "L 325.413636 144.887788 \n",
       "L 328.488017 147.136034 \n",
       "L 331.562397 149.989578 \n",
       "L 334.636777 152.583708 \n",
       "L 337.711157 157.512555 \n",
       "L 340.785537 160.971396 \n",
       "L 343.859917 165.467888 \n",
       "L 346.934298 169.186142 \n",
       "L 350.008678 172.731453 \n",
       "L 353.083058 178.352069 \n",
       "L 356.157438 185.010336 \n",
       "L 359.231818 214.756364 \n",
       "\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_3\">\n",
       "    <path d=\"M 39.65 224.64 \n",
       "L 39.65 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_4\">\n",
       "    <path d=\"M 374.45 224.64 \n",
       "L 374.45 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_5\">\n",
       "    <path d=\"M 39.65 224.64 \n",
       "L 374.45 224.64 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "   <g id=\"patch_6\">\n",
       "    <path d=\"M 39.65 7.2 \n",
       "L 374.45 7.2 \n",
       "\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n",
       "   </g>\n",
       "  </g>\n",
       " </g>\n",
       " <defs>\n",
       "  <clipPath id=\"p6e7cecef2a\">\n",
       "   <rect height=\"217.44\" width=\"334.8\" x=\"39.65\" y=\"7.2\"/>\n",
       "  </clipPath>\n",
       " </defs>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(TT,TP)\n",
    "#plt.plot(TT,FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TPR = np.array(TP)/(np.array(TP)+np.array(FN))\n",
    "FPR = np.array(FP)/(np.array(FP)+np.array(TN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(FPR,TPR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='blue', lw=2, label='ROC curve - sklearn')\n",
    "plt.plot(FPR, TPR, color='red', lw=2, label='ROC curve - hand')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate (FPR)')\n",
    "plt.ylabel('True Positive Rate (TPR)')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#generate alphas from 10^-4 to 10^4\n",
    "alphas = np.logspace(-4,4,9)\n",
    "\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "it=0\n",
    "\n",
    "for alpha in alphas:\n",
    "    #use different color scheme for each alpha\n",
    "    c1 = 1.*( float(it) % 3.)/3.0\n",
    "    c2 = 1.*( float(it) % 9.)/9.0\n",
    "    c3 = 1.*( float(it) % 27.)/27.0\n",
    "    \n",
    "    probs, FPR, TPR = runSciKitNNRegression(8,alpha)\n",
    "    ax.plot(FPR, TPR, c=[c1,c2,c3], label='Alpha: %.1E' %alpha)    \n",
    "    it+=1\n",
    "\n",
    "\n",
    "ax.set_xlabel('FPR')\n",
    "ax.set_ylabel('TPR')\n",
    "plt.legend(loc='best', fontsize = 'medium');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "An accuracy of ~79% is pretty good! More important to a physics analysis is the ROC curve above. This allows physicists to define a part of phase space that has more or less signal acceptance, but also less or more background rejection. It is up to the individual analysts in particular searches to decide on the best working point for them.\n",
    "\n",
    "## Question 1: What does this plot tell you? How would the curve look as you approach a perfect classifier?\n",
    "\n",
    "---\n",
    "\n",
    "As you can see, using just the kinematic variables of the final state objects there is no need for regularization and in fact as we turn on the relative weights of the variables (as alpha goes to 1) we lose discrimination power.\n",
    "\n",
    "## Question 2: Why do you think this is the case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs,fpr,tpr = runSciKitNNRegression(8,10**(-4))\n",
    "\n",
    "X_test, y_test = getTestData(8)\n",
    "signal = probs[:,1][y_test==1]\n",
    "background = probs[:,1][y_test==0]\n",
    "\n",
    "mybins = np.histogram_bin_edges(signal, bins=50, range=(0,1))\n",
    "plt.hist(signal, bins=mybins, label='signal')\n",
    "plt.hist(background, bins=mybins, label='background')\n",
    "plt.legend(loc='best', fontsize = 'medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# same but with PANDAS\n",
    "#probsSimple,fpr,tpr = runSciKitNNRegression(8,10**(-4))\n",
    "#Signal = df_test['signal'].values\n",
    "\n",
    "#df_test_acc = pd.DataFrame({'PROB':probsSimple[:,1]})\n",
    "#df_test_acc['SIG']=Signal\n",
    "#df_test_acc_sig = df_test_acc.query('SIG==1')\n",
    "#df_test_acc_bkg = df_test_acc.query('SIG==0')\n",
    "#df_test_acc_sig.plot(kind='hist',y='PROB',color='blue',alpha=0.5,bins=np.linspace(0,1,50),label='Signal')\n",
    "#df_test_acc_bkg.plot(kind='hist',y='PROB',color='red',alpha=0.5,bins=np.linspace(0,1,50),label='Background')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 3: What does this plot tell you about the classification model?\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training with a Multi-layer Perceptron classifier (all 18 features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4,4,9)\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "it=0\n",
    "for alpha in alphas:\n",
    "    c1 = 1.*( float(it) % 3.)/3.0\n",
    "    c2 = 1.*( float(it) % 9.)/9.0\n",
    "    c3 = 1.*( float(it) % 27.)/27.0\n",
    "    probs,fpr,tpr = runSciKitNNRegression(18,alpha)\n",
    "    ax.plot(fpr,tpr,c=[c1,c2,c3],label='Alpha: %.1E' %alpha)\n",
    "    it+=1\n",
    "\n",
    "ax.set_xlabel('signal efficiency')\n",
    "ax.set_ylabel('background rejection')\n",
    "plt.legend(loc='lower left', fontsize = 'medium');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Interestingly the accuracy barely improved! \n",
    "\n",
    "## Question 4: What can you conclude from this? Compute the difference in execution time between using 8 and 18 features and comment on whether you think it is worth it to use these 10 extra features in this problem.\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "probs,fpr,tpr = runSciKitNNRegression(8,10**(-4))\n",
    "print(\"finished in \",time.time()-start_time)\n",
    "\n",
    "X_test, y_test = getTestData(8)\n",
    "signal = probs[:,1][y_test==1]\n",
    "background = probs[:,1][y_test==0]\n",
    "\n",
    "mybins = np.histogram_bin_edges(signal, bins=50, range=(0,1))\n",
    "plt.hist(signal, bins=mybins, label='signal')\n",
    "plt.hist(background, bins=mybins, label='background')\n",
    "plt.legend(loc='best', fontsize = 'medium')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "probs,fpr,tpr = runSciKitNNRegression(18,10**(-4))\n",
    "print(\"finished in \",time.time()-start_time)\n",
    "\n",
    "X_test, y_test = getTestData(8)\n",
    "signal = probs[:,1][y_test==1]\n",
    "background = probs[:,1][y_test==0]\n",
    "\n",
    "mybins = np.histogram_bin_edges(signal, bins=50, range=(0,1))\n",
    "plt.hist(signal, bins=mybins, label='signal')\n",
    "plt.hist(background, bins=mybins, label='background')\n",
    "plt.legend(loc='best', fontsize = 'medium')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Repeat the same exercice above but now with an SGDClassifier+L2 (directly with all 18 features). Does the Linear Classifer performs as good as Multi-layer Perceptron? Why is that?\n",
    "\n",
    "Note: Please do that in new cells, that would help you compare the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4,4,9)\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "it=0\n",
    "for alpha in alphas:\n",
    "    c1 = 1.*( float(it) % 3.)/3.0\n",
    "    c2 = 1.*( float(it) % 9.)/9.0\n",
    "    c3 = 1.*( float(it) % 27.)/27.0\n",
    "    probs,fpr,tpr = runSciKitRegressionL2(18,alpha)\n",
    "    ax.plot(fpr,tpr,c=[c1,c2,c3],label='Alpha: %.1E' %alpha)\n",
    "    it+=1\n",
    "\n",
    "ax.set_xlabel('signal efficiency')\n",
    "ax.set_ylabel('background rejection')\n",
    "plt.legend(loc='lower left', fontsize = 'medium');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Question 6: Repeat the same exercice above but now with an SGDClassifier+L1 (directly with all 18 features). How Does SGD+L1 compares to SGD+L2?\n",
    "\n",
    "Note: Please do that in new cells, that would help you compare the plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4,4,9)\n",
    "fig = plt.figure(figsize = (8,6))\n",
    "ax = fig.add_subplot(111)\n",
    "it=0\n",
    "for alpha in alphas:\n",
    "    c1 = 1.*( float(it) % 3.)/3.0\n",
    "    c2 = 1.*( float(it) % 9.)/9.0\n",
    "    c3 = 1.*( float(it) % 27.)/27.0\n",
    "    probs,fpr,tpr = runSciKitRegressionL1(18,alpha)\n",
    "    ax.plot(fpr,tpr,c=[c1,c2,c3],label='Alpha: %.1E' %alpha)\n",
    "    it+=1\n",
    "\n",
    "ax.set_xlabel('signal efficiency')\n",
    "ax.set_ylabel('background rejection')\n",
    "plt.legend(loc='lower left', fontsize = 'medium');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7: What conclusions can you draw on the usage of L1 and L2 for this specific dataset?\n",
    "\n",
    "## a) What do these two regularisation methods tell us about the dataset?\n",
    "## b) What do these two regularisation methods tell us about the features of the dataset?\n",
    "## c) Which of these two regularisation methods is more suitable for the classification of this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
